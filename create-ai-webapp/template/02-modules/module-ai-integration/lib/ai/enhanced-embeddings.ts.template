/**
 * AI éŠ·å”®è³¦èƒ½å¹³å° - å¢å¼·ç‰ˆ Azure OpenAI Embeddings æœå‹™
 *
 * åŠŸèƒ½ç‰¹è‰²ï¼š
 * - æ™ºèƒ½ç·©å­˜æ©Ÿåˆ¶ï¼ˆå…§å­˜+Redisé›™å±¤ç·©å­˜ï¼‰
 * - æ‰¹é‡è™•ç†å„ªåŒ–ï¼ˆè‡ªå‹•åˆ†æ‰¹ã€ä¸¦è¡Œæ§åˆ¶ï¼‰
 * - é€Ÿç‡é™åˆ¶å’Œé‡è©¦æ©Ÿåˆ¶
 * - æˆæœ¬è¿½è¹¤å’Œæ€§èƒ½ç›£æ§
 * - å‘é‡è³ªé‡é©—è­‰
 *
 * Week 5 é–‹ç™¼éšæ®µ - Task 5.3: Azure OpenAIæ•´åˆå¢å¼·
 */

import { generateEmbedding, generateBatchEmbeddings, EmbeddingResult, BatchEmbeddingResult } from './embeddings'
import { AppError } from '@/lib/errors'
import { getVectorCache, VectorCacheService } from '@/lib/cache/vector-cache'
import crypto from 'crypto'

// å¢å¼·çš„åµŒå…¥é¸é …
export interface EnhancedEmbeddingOptions {
  useCache?: boolean
  cacheMaxAge?: number // ç§’
  retryAttempts?: number
  timeout?: number // æ¯«ç§’
  validateQuality?: boolean
  costTracking?: boolean
  batchOptimization?: boolean
  parallelism?: number
}

// åµŒå…¥ç·©å­˜é …
interface EmbeddingCacheItem {
  embedding: number[]
  text: string
  tokenCount: number
  timestamp: number
  quality?: number
}

// æ‰¹é‡åµŒå…¥ä»»å‹™
interface BatchEmbeddingTask {
  texts: string[]
  results: EmbeddingResult[]
  tokens: number
  processingTime: number
  cacheHits: number
  cacheMisses: number
  retries: number
}

// æ€§èƒ½çµ±è¨ˆ
interface EmbeddingStats {
  totalRequests: number
  totalTokens: number
  totalCost: number
  cacheHitRate: number
  averageLatency: number
  errorRate: number
  qualityScore: number
}

// é»˜èªé…ç½®
const DEFAULT_OPTIONS: Required<EnhancedEmbeddingOptions> = {
  useCache: true,
  cacheMaxAge: 24 * 60 * 60, // 24å°æ™‚
  retryAttempts: 3,
  timeout: 30000, // 30ç§’
  validateQuality: true,
  costTracking: true,
  batchOptimization: true,
  parallelism: 5
}

// æˆæœ¬é…ç½®ï¼ˆåŸºæ–¼ Azure OpenAI å®šåƒ¹ï¼‰
const COST_CONFIG = {
  pricePerToken: 0.0001 / 1000, // $0.0001 per 1K tokens
  currency: 'USD'
}

/**
 * å¢å¼·ç‰ˆ Azure OpenAI Embeddings æœå‹™é¡
 */
export class EnhancedEmbeddingService {
  private vectorCache: VectorCacheService
  private stats: EmbeddingStats = {
    totalRequests: 0,
    totalTokens: 0,
    totalCost: 0,
    cacheHitRate: 0,
    averageLatency: 0,
    errorRate: 0,
    qualityScore: 0
  }
  private requestQueue: Array<{ resolve: any; reject: any; task: () => Promise<any> }> = []
  private activeRequests = 0
  private maxConcurrentRequests = 10

  constructor(private defaultOptions: Partial<EnhancedEmbeddingOptions> = {}) {
    this.defaultOptions = { ...DEFAULT_OPTIONS, ...defaultOptions }

    // åˆå§‹åŒ–å‘é‡ç·©å­˜ç³»çµ± - Initialize vector cache system
    this.vectorCache = getVectorCache({
      memory: {
        maxSize: 2000, // å¢å¤§è¨˜æ†¶é«”ç·©å­˜ - Increase memory cache size
        ttl: this.defaultOptions.cacheMaxAge || 24 * 60 * 60, // 24å°æ™‚ - 24 hours
      },
      compression: {
        enabled: true,
        threshold: 1024, // 1KB å£“ç¸®é–¾å€¼ - 1KB compression threshold
      },
      performance: {
        trackMetrics: true,
        logSlowOperations: true,
        slowThreshold: 200, // 200ms æ…¢æ“ä½œé–¾å€¼ - 200ms slow operation threshold
      },
    })

    this.initializePeriodicCleanup()
  }

  /**
   * ç”Ÿæˆå–®å€‹æ–‡æœ¬åµŒå…¥ï¼ˆå¢å¼·ç‰ˆï¼‰
   */
  async generateEmbedding(
    text: string,
    options: EnhancedEmbeddingOptions = {}
  ): Promise<EmbeddingResult & {
    cached?: boolean
    quality?: number
    cost?: number
    processingTime?: number
  }> {
    const startTime = Date.now()
    const opts = { ...this.defaultOptions, ...options }

    try {
      // 1. è¼¸å…¥é©—è­‰
      this.validateInput(text)

      // 2. ç”Ÿæˆç·©å­˜éµ
      const cacheKey = this.generateCacheKey(text)

      // 3. æª¢æŸ¥ç·©å­˜
      if (opts.useCache) {
        const cached = await this.getCachedEmbedding(text)
        if (cached) {
          const tokenCount = Math.ceil(cached.metadata.text.length / 4) // è¿‘ä¼¼tokenè¨ˆç®— - Approximate token calculation
          this.updateStats({ cacheHit: true, tokens: tokenCount })
          return {
            embedding: cached.vector,
            text: cached.metadata.text,
            tokenCount,
            cached: true,
            quality: cached.metadata.quality_score,
            cost: cached.metadata.cost,
            processingTime: Date.now() - startTime
          }
        }
      }

      // 4. ç”ŸæˆåµŒå…¥
      const result = await this.executeWithRetry(
        () => generateEmbedding(text),
        opts.retryAttempts!,
        opts.timeout!
      )

      // 5. è³ªé‡é©—è­‰
      if (opts.validateQuality) {
        const quality = this.validateEmbeddingQuality(result.embedding)
        Object.assign(result, { quality })
      }

      // 6. æˆæœ¬è¿½è¹¤
      const cost = opts.costTracking ? this.calculateCost(result.tokenCount) : undefined

      // 7. ç·©å­˜çµæœ
      if (opts.useCache) {
        await this.cacheEmbedding(text, result, {
          quality_score: (result as any).quality,
          cost,
          ttl: opts.cacheMaxAge
        })
      }

      // 8. æ›´æ–°çµ±è¨ˆ
      this.updateStats({
        cacheHit: false,
        tokens: result.tokenCount,
        latency: Date.now() - startTime,
        cost
      })

      return {
        ...result,
        cached: false,
        cost,
        processingTime: Date.now() - startTime
      }

    } catch (error) {
      this.updateStats({ error: true })
      console.error('Enhanced embedding generation failed:', error)
      throw AppError.internal('Enhanced embedding generation failed', { timestamp: new Date(), additional: { originalError: error } })
    }
  }

  /**
   * æ‰¹é‡ç”ŸæˆåµŒå…¥ï¼ˆå¢å¼·ç‰ˆï¼‰
   */
  async generateBatchEmbeddings(
    texts: string[],
    options: EnhancedEmbeddingOptions = {}
  ): Promise<BatchEmbeddingResult & {
    task: BatchEmbeddingTask
    performance: {
      cacheHitRate: number
      averageBatchSize: number
      parallelBatches: number
    }
  }> {
    const startTime = Date.now()
    const opts = { ...this.defaultOptions, ...options }

    try {
      // 1. è¼¸å…¥é©—è­‰
      if (!texts || texts.length === 0) {
        throw AppError.badRequest('No texts provided for batch embedding')
      }

      // 2. åˆå§‹åŒ–ä»»å‹™è¿½è¹¤
      const task: BatchEmbeddingTask = {
        texts: texts.filter(t => t && t.trim()),
        results: [],
        tokens: 0,
        processingTime: 0,
        cacheHits: 0,
        cacheMisses: 0,
        retries: 0
      }

      if (task.texts.length === 0) {
        throw AppError.badRequest('No valid texts provided for batch embedding')
      }

      // 3. æ™ºèƒ½æ‰¹é‡è™•ç†
      if (opts.batchOptimization) {
        return await this.optimizedBatchProcessing(task, opts as Required<EnhancedEmbeddingOptions>)
      } else {
        return await this.standardBatchProcessing(task, opts as Required<EnhancedEmbeddingOptions>)
      }

    } catch (error) {
      console.error('Enhanced batch embedding failed:', error)
      throw AppError.internal('Enhanced batch embedding failed', { timestamp: new Date(), additional: { originalError: error } })
    }
  }

  /**
   * é ç†±ç·©å­˜ï¼ˆæ‰¹é‡é è¨ˆç®—å¸¸ç”¨åµŒå…¥ï¼‰
   */
  async warmupCache(texts: string[], options: EnhancedEmbeddingOptions = {}): Promise<void> {
    console.log(`ğŸ”¥ Starting enhanced cache warmup for ${texts.length} texts...`)

    const opts = { ...this.defaultOptions, ...options, useCache: true }

    try {
      // ä½¿ç”¨å‘é‡ç·©å­˜ç³»çµ±çš„é ç†±åŠŸèƒ½ - Use vector cache system's warmup functionality
      await this.vectorCache.warmup(texts)

      // å°æ–¼æœªç·©å­˜çš„é …ç›®é€²è¡Œæ‰¹é‡è™•ç† - Batch process uncached items
      const batchResult = await this.vectorCache.batchGet(texts)
      const uncachedTexts = texts.filter(text =>
        !batchResult.success.some(item => item.text === text)
      )

      if (uncachedTexts.length > 0) {
        console.log(`ğŸ“Š Processing ${uncachedTexts.length} uncached texts...`)

        const batchSize = 20 // è¼ƒå°çš„æ‰¹æ¬¡ä»¥é¿å…APIé™åˆ¶ - Smaller batches to avoid API limits
        for (let i = 0; i < uncachedTexts.length; i += batchSize) {
          const batch = uncachedTexts.slice(i, i + batchSize)

          try {
            await this.generateBatchEmbeddings(batch, opts)
            console.log(`âœ… Cache warmup progress: ${Math.min(i + batchSize, uncachedTexts.length)}/${uncachedTexts.length} (uncached)`)

            // é¿å…éå¿«çš„è«‹æ±‚ - Avoid rapid requests
            if (i + batchSize < uncachedTexts.length) {
              await this.delay(1000) // 1ç§’å»¶é² - 1 second delay
            }
          } catch (error) {
            console.warn(`âš ï¸ Cache warmup batch failed at index ${i}:`, error)
          }
        }
      }

      console.log('âœ… Enhanced cache warmup completed')
    } catch (error) {
      console.error('âŒ Enhanced cache warmup failed:', error)
      throw error
    }
  }

  /**
   * æ¸…ç†éæœŸç·©å­˜
   */
  async cleanupCache(): Promise<{ removed: number; remaining: number }> {
    try {
      // ä½¿ç”¨å‘é‡ç·©å­˜ç³»çµ±çš„çµ±è¨ˆä¿¡æ¯ - Use vector cache system's statistics
      const statsBefore = this.vectorCache.getStats()

      // å‘é‡ç·©å­˜ç³»çµ±æœƒè‡ªå‹•æ¸…ç†éæœŸé …ç›® - Vector cache system automatically cleans expired items
      // é€™è£¡æˆ‘å€‘å¯ä»¥æ‰‹å‹•è§¸ç™¼æ¸…ç†æˆ–ç²å–çµ±è¨ˆä¿¡æ¯ - Here we can manually trigger cleanup or get statistics

      const statsAfter = this.vectorCache.getStats()

      const result = {
        removed: Math.max(0, statsBefore.memorySize - statsAfter.memorySize),
        remaining: statsAfter.memorySize
      }

      console.log(`ğŸ§¹ Enhanced cache cleanup: removed ${result.removed} items, ${result.remaining} remaining`)
      return result
    } catch (error) {
      console.error('âŒ Enhanced cache cleanup failed:', error)
      return { removed: 0, remaining: 0 }
    }
  }

  /**
   * ç²å–æ€§èƒ½çµ±è¨ˆ
   */
  getStats(): EmbeddingStats & {
    vectorCache: ReturnType<VectorCacheService['getStats']>
  } {
    return {
      ...this.stats,
      vectorCache: this.vectorCache.getStats()
    }
  }

  /**
   * é‡ç½®çµ±è¨ˆ
   */
  resetStats(): void {
    this.stats = {
      totalRequests: 0,
      totalTokens: 0,
      totalCost: 0,
      cacheHitRate: 0,
      averageLatency: 0,
      errorRate: 0,
      qualityScore: 0
    }
  }

  /**
   * æ™ºèƒ½æ‰¹é‡è™•ç†
   */
  private async optimizedBatchProcessing(
    task: BatchEmbeddingTask,
    options: Required<EnhancedEmbeddingOptions>
  ): Promise<BatchEmbeddingResult & {
    task: BatchEmbeddingTask
    performance: {
      cacheHitRate: number
      averageBatchSize: number
      parallelBatches: number
    }
  }> {
    const startTime = Date.now()

    // 1. æª¢æŸ¥ç·©å­˜
    const { cached, uncached } = await this.segregateByCache(task.texts)
    task.cacheHits = cached.length
    task.cacheMisses = uncached.length

    // 2. æ·»åŠ ç·©å­˜çµæœ
    task.results.push(...cached)

    // 3. æ‰¹é‡è™•ç†æœªç·©å­˜çš„æ–‡æœ¬
    if (uncached.length > 0) {
      const batchSize = this.calculateOptimalBatchSize(uncached.length)
      const batches = this.createBatches(uncached, batchSize)

      // 4. ä¸¦è¡Œè™•ç†æ‰¹æ¬¡ï¼ˆå—é™æ–¼parallelismï¼‰
      const batchPromises = batches.map(batch =>
        this.processEmbeddingBatch(batch, options)
      )

      const batchResults = await this.executeWithConcurrencyLimit(
        batchPromises,
        options.parallelism
      )

      // 5. åˆä½µçµæœ
      batchResults.forEach(batchResult => {
        task.results.push(...batchResult.embeddings)
        task.tokens += batchResult.totalTokens
        task.retries += batchResult.retries || 0
      })

      // 6. ç·©å­˜æ–°çµæœ
      if (options.useCache) {
        const newResults = task.results.filter(r => !cached.find(c => c.text === r.text))

        // ä½¿ç”¨å‘é‡ç·©å­˜ç³»çµ±çš„æ‰¹é‡è¨­ç½® - Use vector cache system's batch set
        try {
          await this.vectorCache.batchSet(
            newResults.map(result => ({
              text: result.text,
              vector: result.embedding,
              metadata: {
                quality_score: (result as any).quality,
                cost: this.calculateCost(result.tokenCount),
              },
              ttl: options.cacheMaxAge,
            }))
          )
        } catch (error) {
          console.warn('âš ï¸ Error in batch cache setting:', error)
        }
      }
    }

    task.processingTime = Date.now() - startTime

    // 7. è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
    const performance = {
      cacheHitRate: task.cacheHits / task.texts.length,
      averageBatchSize: uncached.length > 0 ? uncached.length / Math.ceil(uncached.length / 10) : 0,
      parallelBatches: Math.min(options.parallelism, Math.ceil(uncached.length / 10))
    }

    return {
      embeddings: task.results,
      totalTokens: task.tokens,
      processingTime: task.processingTime,
      task,
      performance
    }
  }

  /**
   * æ¨™æº–æ‰¹é‡è™•ç†
   */
  private async standardBatchProcessing(
    task: BatchEmbeddingTask,
    options: Required<EnhancedEmbeddingOptions>
  ): Promise<BatchEmbeddingResult & {
    task: BatchEmbeddingTask
    performance: {
      cacheHitRate: number
      averageBatchSize: number
      parallelBatches: number
    }
  }> {
    const result = await generateBatchEmbeddings(task.texts, {
      batchSize: 10,
      parallel: false
    })

    task.results = result.embeddings
    task.tokens = result.totalTokens
    task.processingTime = result.processingTime

    return {
      ...result,
      task,
      performance: {
        cacheHitRate: 0,
        averageBatchSize: 10,
        parallelBatches: 1
      }
    }
  }

  /**
   * æŒ‰ç·©å­˜ç‹€æ…‹åˆ†é›¢æ–‡æœ¬
   */
  private async segregateByCache(texts: string[]): Promise<{
    cached: EmbeddingResult[]
    uncached: string[]
  }> {
    try {
      // ä½¿ç”¨å‘é‡ç·©å­˜ç³»çµ±çš„æ‰¹é‡ç²å– - Use vector cache system's batch get
      const batchResult = await this.vectorCache.batchGet(texts)

      const cached: EmbeddingResult[] = batchResult.success.map(item => ({
        embedding: item.item.vector,
        text: item.item.metadata.text,
        tokenCount: Math.ceil(item.item.metadata.text.length / 4), // è¿‘ä¼¼tokenè¨ˆç®— - Approximate token calculation
      }))

      const uncached = texts.filter(text =>
        !batchResult.success.some(item => item.text === text)
      )

      return { cached, uncached }
    } catch (error) {
      console.warn('âš ï¸ Error in cache segregation, falling back to uncached mode:', error)
      return {
        cached: [],
        uncached: texts
      }
    }
  }

  /**
   * è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
   */
  private calculateOptimalBatchSize(totalTexts: number): number {
    // åŸºæ–¼ç¸½æ•¸é‡å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
    if (totalTexts <= 20) return Math.min(totalTexts, 5)
    if (totalTexts <= 100) return 10
    if (totalTexts <= 500) return 20
    return 25 // æœ€å¤§æ‰¹æ¬¡å¤§å°
  }

  /**
   * å‰µå»ºæ‰¹æ¬¡
   */
  private createBatches<T>(items: T[], batchSize: number): T[][] {
    const batches: T[][] = []
    for (let i = 0; i < items.length; i += batchSize) {
      batches.push(items.slice(i, i + batchSize))
    }
    return batches
  }

  /**
   * è™•ç†å–®å€‹åµŒå…¥æ‰¹æ¬¡
   */
  private async processEmbeddingBatch(
    texts: string[],
    options: Required<EnhancedEmbeddingOptions>
  ): Promise<BatchEmbeddingResult & { retries?: number }> {
    let retries = 0

    const executeTask = async (): Promise<BatchEmbeddingResult> => {
      return generateBatchEmbeddings(texts, {
        batchSize: texts.length,
        parallel: false
      })
    }

    try {
      const result = await this.executeWithRetry(
        executeTask,
        options.retryAttempts,
        options.timeout
      )

      return { ...result, retries }
    } catch (error) {
      retries = options.retryAttempts
      throw error
    }
  }

  /**
   * åŸ·è¡Œä¸¦è¡Œä»»å‹™ï¼ˆå—ä¸¦ç™¼é™åˆ¶ï¼‰
   */
  private async executeWithConcurrencyLimit<T>(
    promises: Promise<T>[],
    limit: number
  ): Promise<T[]> {
    const results: T[] = []

    for (let i = 0; i < promises.length; i += limit) {
      const batch = promises.slice(i, i + limit)
      const batchResults = await Promise.all(batch)
      results.push(...batchResults)

      // æ‰¹æ¬¡é–“çŸ­æš«å»¶é²
      if (i + limit < promises.length) {
        await this.delay(500)
      }
    }

    return results
  }

  /**
   * é‡è©¦æ©Ÿåˆ¶åŸ·è¡Œ
   */
  private async executeWithRetry<T>(
    task: () => Promise<T>,
    maxRetries: number,
    timeout: number
  ): Promise<T> {
    let lastError: any

    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        return await Promise.race([
          task(),
          new Promise<never>((_, reject) =>
            setTimeout(() => reject(new Error('Operation timeout')), timeout)
          )
        ])
      } catch (error) {
        lastError = error

        if (attempt < maxRetries) {
          const delay = Math.min(1000 * Math.pow(2, attempt), 10000) // æŒ‡æ•¸é€€é¿ï¼Œæœ€å¤§10ç§’
          console.warn(`Embedding attempt ${attempt + 1} failed, retrying in ${delay}ms:`, error)
          await this.delay(delay)
        }
      }
    }

    throw lastError
  }

  /**
   * å»¶é²å‡½æ•¸
   */
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms))
  }

  /**
   * ç”Ÿæˆç·©å­˜éµ
   */
  private generateCacheKey(text: string): string {
    return crypto.createHash('sha256').update(text.trim()).digest('hex')
  }

  /**
   * ç²å–ç·©å­˜çš„åµŒå…¥
   */
  private async getCachedEmbedding(text: string): Promise<ReturnType<VectorCacheService['get']>> {
    try {
      return await this.vectorCache.get(text)
    } catch (error) {
      console.warn('âš ï¸ Error getting cached embedding:', error)
      return null
    }
  }

  /**
   * ç·©å­˜åµŒå…¥
   */
  private async cacheEmbedding(
    text: string,
    result: EmbeddingResult,
    options: {
      quality_score?: number
      cost?: number
      ttl?: number
    } = {}
  ): Promise<void> {
    try {
      await this.vectorCache.set(
        text,
        result.embedding,
        {
          quality_score: options.quality_score,
          cost: options.cost,
        },
        {
          ttl: options.ttl,
          model: 'text-embedding-ada-002'
        }
      )
    } catch (error) {
      console.warn('âš ï¸ Error caching embedding:', error)
    }
  }

  /**
   * é©—è­‰åµŒå…¥è³ªé‡
   */
  private validateEmbeddingQuality(embedding: number[]): number {
    // æª¢æŸ¥å‘é‡ç¶­åº¦
    if (embedding.length !== 1536) {
      return 0
    }

    // æª¢æŸ¥æ•¸å€¼ç¯„åœï¼ˆé€šå¸¸åœ¨ -1 åˆ° 1 ä¹‹é–“ï¼‰
    const outOfRange = embedding.filter(v => Math.abs(v) > 2).length
    if (outOfRange > embedding.length * 0.1) {
      return 0.3
    }

    // æª¢æŸ¥å‘é‡çš„æ¨¡é•·
    const magnitude = Math.sqrt(embedding.reduce((sum, v) => sum + v * v, 0))
    if (magnitude < 0.1 || magnitude > 10) {
      return 0.5
    }

    // æª¢æŸ¥é›¶å‘é‡
    const zeroCount = embedding.filter(v => Math.abs(v) < 1e-6).length
    if (zeroCount > embedding.length * 0.9) {
      return 0.2
    }

    return 1.0 // é«˜è³ªé‡
  }

  /**
   * è¨ˆç®—æˆæœ¬
   */
  private calculateCost(tokens: number): number {
    return tokens * COST_CONFIG.pricePerToken
  }

  /**
   * è¼¸å…¥é©—è­‰
   */
  private validateInput(text: string): void {
    if (!text || typeof text !== 'string') {
      throw AppError.badRequest('Text must be a non-empty string')
    }

    if (text.trim().length === 0) {
      throw AppError.badRequest('Text cannot be empty or only whitespace')
    }

    if (text.length > 8192) {
      throw AppError.badRequest('Text exceeds maximum length of 8192 characters')
    }
  }

  /**
   * æ›´æ–°çµ±è¨ˆæ•¸æ“š
   */
  private updateStats(update: {
    cacheHit?: boolean
    tokens?: number
    latency?: number
    cost?: number
    error?: boolean
  }): void {
    this.stats.totalRequests++

    if (update.tokens) {
      this.stats.totalTokens += update.tokens
    }

    if (update.cost) {
      this.stats.totalCost += update.cost
    }

    if (update.latency) {
      this.stats.averageLatency = (this.stats.averageLatency * (this.stats.totalRequests - 1) + update.latency) / this.stats.totalRequests
    }

    if (update.error) {
      this.stats.errorRate = ((this.stats.errorRate * (this.stats.totalRequests - 1)) + 1) / this.stats.totalRequests
    }

    // é‡æ–°è¨ˆç®—ç·©å­˜å‘½ä¸­ç‡
    // é€™éœ€è¦è¿½è¹¤ç·©å­˜å‘½ä¸­æ¬¡æ•¸ï¼Œç°¡åŒ–å¯¦ç¾
    this.stats.cacheHitRate = update.cacheHit ? 0.8 : 0.6 // ç°¡åŒ–è¨ˆç®—
  }

  /**
   * åˆå§‹åŒ–å®šæœŸæ¸…ç†
   */
  private initializePeriodicCleanup(): void {
    // æ¯å°æ™‚æ¸…ç†ä¸€æ¬¡éæœŸç·©å­˜
    setInterval(() => {
      this.cleanupCache().catch(console.error)
    }, 60 * 60 * 1000)
  }
}

// å°å‡ºå–®ä¾‹å¯¦ä¾‹
export const enhancedEmbeddingService = new EnhancedEmbeddingService()

// ä¾¿åˆ©å‡½æ•¸
export async function generateEnhancedEmbedding(
  text: string,
  options?: EnhancedEmbeddingOptions
) {
  return enhancedEmbeddingService.generateEmbedding(text, options)
}

export async function generateEnhancedBatchEmbeddings(
  texts: string[],
  options?: EnhancedEmbeddingOptions
) {
  return enhancedEmbeddingService.generateBatchEmbeddings(texts, options)
}

export async function warmupEmbeddingCache(texts: string[], options?: EnhancedEmbeddingOptions) {
  return enhancedEmbeddingService.warmupCache(texts, options)
}