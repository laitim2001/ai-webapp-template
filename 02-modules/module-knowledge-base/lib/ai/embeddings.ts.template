/**
 * ================================================================
 * Vector Embedding Service
 * ================================================================
 *
 * Provides text vector embedding services for semantic search and document retrieval.
 * Supports single embeddings, batch processing, document chunking, and similarity calculation.
 *
 * Features:
 * • Single text vector embedding - Convert text to vector representation
 * • Batch text embedding - Process multiple texts efficiently
 * • Long document chunking - Split long documents intelligently
 * • Similarity calculation - Cosine similarity for semantic matching
 * • Error handling and retry - Complete error recovery mechanism
 *
 * Embedding Provider Configuration:
 * Set EMBEDDING_SERVICE in .env.local:
 * - "openai": OpenAI embeddings API
 * - "azure": Azure OpenAI Service
 * - "custom": Custom embedding service (implement your own)
 *
 * Author: AI Web App Template
 * Version: 5.0
 */

// Embedding configuration constants
const MAX_CHUNK_SIZE = 8192; // Maximum token limit for text-embedding models
const EMBEDDING_DIMENSION = 1536; // Vector dimension for text-embedding-ada-002

/**
 * Embedding Service Configuration Interface
 */
export interface EmbeddingServiceConfig {
  provider: 'openai' | 'azure' | 'custom';
  apiKey?: string;
  endpoint?: string;
  deploymentId?: string;
  model?: string;
}

/**
 * Single text embedding result interface
 */
export interface EmbeddingResult {
  embedding: number[]; // Vector representation (1536 dimensions)
  text: string; // Original input text
  tokenCount: number; // Token count consumed
}

/**
 * Batch embedding result interface
 */
export interface BatchEmbeddingResult {
  embeddings: EmbeddingResult[]; // Array of all embedding results
  totalTokens: number; // Total tokens consumed
  processingTime: number; // Total processing time (ms)
}

/**
 * Get embedding service configuration from environment variables
 */
function getEmbeddingConfig(): EmbeddingServiceConfig {
  const provider = (process.env.EMBEDDING_SERVICE || 'openai') as 'openai' | 'azure' | 'custom';

  return {
    provider,
    apiKey: process.env.OPENAI_API_KEY || process.env.AZURE_OPENAI_API_KEY,
    endpoint: process.env.AZURE_OPENAI_ENDPOINT,
    deploymentId: process.env.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
    model: process.env.EMBEDDING_MODEL || 'text-embedding-ada-002',
  };
}

/**
 * Call embedding API based on provider configuration
 *
 * @param texts Array of texts to embed
 * @returns Promise with embedding data
 * @private
 */
async function callEmbeddingAPI(texts: string[]): Promise<any> {
  const config = getEmbeddingConfig();

  switch (config.provider) {
    case 'openai':
      return callOpenAIEmbedding(texts, config);

    case 'azure':
      return callAzureOpenAIEmbedding(texts, config);

    case 'custom':
      return callCustomEmbedding(texts, config);

    default:
      throw new Error(`Unsupported embedding provider: ${config.provider}`);
  }
}

/**
 * Call OpenAI Embeddings API
 * @private
 */
async function callOpenAIEmbedding(texts: string[], config: EmbeddingServiceConfig): Promise<any> {
  if (!config.apiKey) {
    throw new Error('OPENAI_API_KEY is required for OpenAI provider');
  }

  const response = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${config.apiKey}`,
    },
    body: JSON.stringify({
      input: texts,
      model: config.model || 'text-embedding-ada-002',
    }),
  });

  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.statusText}`);
  }

  return response.json();
}

/**
 * Call Azure OpenAI Embeddings API
 * @private
 */
async function callAzureOpenAIEmbedding(
  texts: string[],
  config: EmbeddingServiceConfig
): Promise<any> {
  if (!config.endpoint || !config.apiKey || !config.deploymentId) {
    throw new Error(
      'AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, and AZURE_OPENAI_EMBEDDING_DEPLOYMENT are required for Azure provider'
    );
  }

  const url = `${config.endpoint}/openai/deployments/${config.deploymentId}/embeddings?api-version=2023-05-15`;

  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'api-key': config.apiKey,
    },
    body: JSON.stringify({
      input: texts,
    }),
  });

  if (!response.ok) {
    throw new Error(`Azure OpenAI API error: ${response.statusText}`);
  }

  return response.json();
}

/**
 * Call Custom Embedding Service
 * IMPLEMENT YOUR CUSTOM EMBEDDING LOGIC HERE
 * @private
 */
async function callCustomEmbedding(texts: string[], config: EmbeddingServiceConfig): Promise<any> {
  // TODO: Implement your custom embedding service
  // Example: Call your self-hosted embedding model or third-party service
  throw new Error('Custom embedding service not implemented. Please implement callCustomEmbedding()');
}

/**
 * Generate vector embedding for single text
 *
 * Converts input text to vector representation using configured embedding service.
 * Includes complete error handling and retry mechanism.
 *
 * @param text Text content to convert to vector
 * @returns Promise<EmbeddingResult> Result containing vector, text, and token count
 * @throws Error when text is empty or API call fails
 *
 * @example
 * ```typescript
 * const result = await generateEmbedding("This is a test document");
 * console.log(result.embedding.length); // 1536
 * console.log(result.tokenCount); // ~6
 * ```
 */
export async function generateEmbedding(text: string): Promise<EmbeddingResult> {
  // Input validation: check if text is empty
  if (!text || text.trim().length === 0) {
    throw new Error('Text cannot be empty for embedding generation');
  }

  try {
    const cleanText = text.trim();

    // Call embedding API
    const response = await callEmbeddingAPI([cleanText]);

    // Validate API response completeness
    if (!response.data || response.data.length === 0) {
      throw new Error('No embedding data received from embedding service');
    }

    const embeddingData = response.data[0];

    // Build result object
    return {
      embedding: embeddingData.embedding, // Vector representation
      text: cleanText, // Cleaned original text
      tokenCount: response.usage?.total_tokens || 0, // Token usage
    };
  } catch (error) {
    console.error('Error generating embedding:', error);
    throw new Error(`Failed to generate embedding: ${error}`);
  }
}

/**
 * Generate vector embeddings for multiple texts in batch
 *
 * Efficiently process multiple text vectorization needs with batch processing.
 * Suitable for large-scale document processing scenarios.
 *
 * @param texts Array of texts to process
 * @param options Batch processing options
 * @param options.batchSize Number of texts per batch (default: 10)
 * @param options.parallel Whether to process batches in parallel (default: false)
 * @returns Promise<BatchEmbeddingResult> All embedding results and statistics
 * @throws Error when input is empty or API call fails
 *
 * @example
 * ```typescript
 * const texts = ["Document 1", "Document 2", "Document 3"];
 * const result = await generateBatchEmbeddings(texts, {
 *   batchSize: 5,
 *   parallel: false
 * });
 * console.log(`Processed ${result.embeddings.length} documents`);
 * ```
 */
export async function generateBatchEmbeddings(
  texts: string[],
  options: {
    batchSize?: number;
    parallel?: boolean;
  } = {}
): Promise<BatchEmbeddingResult> {
  const { batchSize = 10, parallel = false } = options;
  const startTime = Date.now();

  // Handle empty array
  if (texts.length === 0) {
    return {
      embeddings: [],
      totalTokens: 0,
      processingTime: 0,
    };
  }

  // Filter empty and invalid texts
  const validTexts = texts.filter((text) => text && text.trim().length > 0);

  if (validTexts.length === 0) {
    throw new Error('No valid texts provided for embedding generation');
  }

  try {
    const allResults: EmbeddingResult[] = [];
    let totalTokens = 0;

    // Split texts into batches
    const batches = [];
    for (let i = 0; i < validTexts.length; i += batchSize) {
      batches.push(validTexts.slice(i, i + batchSize));
    }

    /**
     * Process a single batch of embeddings
     */
    const processBatch = async (batch: string[]): Promise<EmbeddingResult[]> => {
      const response = await callEmbeddingAPI(batch);

      if (!response.data) {
        throw new Error('No embedding data received from embedding service');
      }

      // Convert API response to standard format and distribute token usage
      const batchResults = response.data.map((embeddingData: any, index: number) => ({
        embedding: embeddingData.embedding,
        text: batch[index].trim(),
        // Distribute total token usage evenly across batch texts
        tokenCount: Math.ceil((response.usage?.total_tokens || 0) / batch.length),
      }));

      totalTokens += response.usage?.total_tokens || 0;
      return batchResults;
    };

    // Process based on parallel option
    if (parallel) {
      // Parallel processing - faster but may trigger rate limits
      const batchPromises = batches.map(processBatch);
      const batchResults = await Promise.all(batchPromises);
      batchResults.forEach((results) => allResults.push(...results));
    } else {
      // Sequential processing - slower but avoids rate limits
      for (const batch of batches) {
        const results = await processBatch(batch);
        allResults.push(...results);
      }
    }

    const processingTime = Date.now() - startTime;

    return {
      embeddings: allResults,
      totalTokens,
      processingTime,
    };
  } catch (error) {
    console.error('Error generating batch embeddings:', error);
    throw new Error(`Failed to generate batch embeddings: ${error}`);
  }
}

/**
 * Smart text chunking (for long documents)
 *
 * Splits long documents into chunks suitable for embedding processing.
 * Uses intelligent splitting algorithm to ensure semantic integrity.
 * Priority for split points: Period > Newline > Space
 *
 * @param text Original text to split
 * @param chunkSize Maximum characters per chunk (default: 8192)
 * @param overlapSize Overlap characters between chunks (default: 200)
 * @returns string[] Array of text chunks
 *
 * @example
 * ```typescript
 * const longText = "Very long document content...";
 * const chunks = splitTextIntoChunks(longText, 1000, 100);
 * console.log(`Document split into ${chunks.length} chunks`);
 * ```
 */
export function splitTextIntoChunks(
  text: string,
  chunkSize: number = MAX_CHUNK_SIZE,
  overlapSize: number = 200
): string[] {
  // Handle empty text
  if (!text || text.trim().length === 0) {
    return [];
  }

  const cleanText = text.trim();
  const chunks: string[] = [];

  // If text is smaller than chunk size, return whole text
  if (cleanText.length <= chunkSize) {
    return [cleanText];
  }

  let startIndex = 0;

  // Loop to split text until all content is processed
  while (startIndex < cleanText.length) {
    let endIndex = startIndex + chunkSize;

    // If not the last chunk, find intelligent split point
    if (endIndex < cleanText.length) {
      const searchEnd = Math.min(endIndex + 100, cleanText.length);

      // Initialize best split index as current end position
      let bestSplitIndex = endIndex;

      // Priority 1: Split after period (preserve sentence integrity)
      const periodIndex = cleanText.lastIndexOf('.', searchEnd);
      if (periodIndex > startIndex + chunkSize * 0.8) {
        bestSplitIndex = periodIndex + 1;
      } else {
        // Priority 2: Split after newline (preserve paragraph integrity)
        const newlineIndex = cleanText.lastIndexOf('\n', searchEnd);
        if (newlineIndex > startIndex + chunkSize * 0.8) {
          bestSplitIndex = newlineIndex + 1;
        } else {
          // Priority 3: Split at space (avoid word separation)
          const spaceIndex = cleanText.lastIndexOf(' ', searchEnd);
          if (spaceIndex > startIndex + chunkSize * 0.8) {
            bestSplitIndex = spaceIndex + 1;
          }
        }
      }

      endIndex = bestSplitIndex;
    }

    // Extract current chunk and trim whitespace
    const chunk = cleanText.substring(startIndex, endIndex).trim();
    if (chunk.length > 0) {
      chunks.push(chunk);
    }

    // Set next chunk start position (include overlap to preserve context)
    startIndex = Math.max(startIndex + 1, endIndex - overlapSize);
  }

  return chunks;
}

/**
 * Complete vectorization for long documents
 *
 * Handles vectorization needs for very long documents with automatic chunking and batch embedding.
 * Suitable for PDF, Word documents, articles, and other long-text content for semantic search.
 *
 * @param text Complete document text to process
 * @param options Document processing options
 * @param options.chunkSize Maximum characters per chunk (default: 8192)
 * @param options.overlapSize Overlap characters between chunks (default: 200)
 * @param options.batchSize Number of chunks per batch (default: 10)
 * @param options.includeMetadata Whether to include position metadata (default: false)
 * @returns Promise with all chunk embeddings and statistics
 *
 * @example
 * ```typescript
 * const documentText = "Long document content...";
 * const result = await generateDocumentEmbeddings(documentText, {
 *   chunkSize: 1000,
 *   includeMetadata: true
 * });
 * console.log(`Document split into ${result.totalChunks} chunks`);
 * ```
 */
export async function generateDocumentEmbeddings(
  text: string,
  options: {
    chunkSize?: number;
    overlapSize?: number;
    batchSize?: number;
    includeMetadata?: boolean;
  } = {}
): Promise<{
  embeddings: Array<
    EmbeddingResult & {
      chunkIndex: number;
      startPosition?: number;
      endPosition?: number;
    }
  >;
  totalChunks: number;
  totalTokens: number;
  processingTime: number;
}> {
  const { chunkSize = MAX_CHUNK_SIZE, overlapSize = 200, batchSize = 10, includeMetadata = false } =
    options;

  const startTime = Date.now();

  // Step 1: Smart chunking
  const chunks = splitTextIntoChunks(text, chunkSize, overlapSize);

  // Handle empty document
  if (chunks.length === 0) {
    return {
      embeddings: [],
      totalChunks: 0,
      totalTokens: 0,
      processingTime: Date.now() - startTime,
    };
  }

  // Step 2: Batch generate embeddings for all chunks
  const batchResult = await generateBatchEmbeddings(chunks, { batchSize });

  // Step 3: Add chunk-level metadata to each embedding result
  const embeddings = batchResult.embeddings.map((embedding, index) => {
    const result: EmbeddingResult & {
      chunkIndex: number;
      startPosition?: number;
      endPosition?: number;
    } = {
      ...embedding,
      chunkIndex: index, // Chunk sequence number in document
    };

    // If metadata needed, calculate chunk position in original text
    if (includeMetadata) {
      const chunkText = embedding.text;
      const startPos = text.indexOf(chunkText);
      if (startPos !== -1) {
        result.startPosition = startPos;
        result.endPosition = startPos + chunkText.length;
      }
    }

    return result;
  });

  return {
    embeddings, // All chunk embeddings (with metadata)
    totalChunks: chunks.length, // Total chunk count
    totalTokens: batchResult.totalTokens, // Total token usage
    processingTime: Date.now() - startTime, // Total processing time
  };
}

/**
 * Calculate cosine similarity between two vectors
 *
 * Cosine similarity measures the similarity of two vector directions, ranging from -1 to 1.
 * 1 = identical, 0 = orthogonal (unrelated), -1 = opposite.
 * Widely used for text semantic similarity comparison and search ranking.
 *
 * @param vectorA First vector (usually query text embedding)
 * @param vectorB Second vector (usually document embedding)
 * @returns number Cosine similarity value (-1 to 1)
 * @throws Error when vector dimensions do not match
 *
 * @example
 * ```typescript
 * const queryEmbedding = await generateEmbedding("search query");
 * const docEmbedding = await generateEmbedding("document content");
 *
 * const similarity = calculateCosineSimilarity(
 *   queryEmbedding.embedding,
 *   docEmbedding.embedding
 * );
 *
 * if (similarity > 0.8) {
 *   console.log("Highly relevant document");
 * } else if (similarity > 0.5) {
 *   console.log("Moderately relevant document");
 * } else {
 *   console.log("Low relevance document");
 * }
 * ```
 */
export function calculateCosineSimilarity(vectorA: number[], vectorB: number[]): number {
  // Check if vector dimensions match
  if (vectorA.length !== vectorB.length) {
    throw new Error('Vectors must have the same dimension');
  }

  let dotProduct = 0; // Dot product accumulator
  let magnitudeA = 0; // Vector A magnitude squared accumulator
  let magnitudeB = 0; // Vector B magnitude squared accumulator

  // Single pass to calculate all needed values
  for (let i = 0; i < vectorA.length; i++) {
    dotProduct += vectorA[i] * vectorB[i]; // Accumulate dot product
    magnitudeA += vectorA[i] * vectorA[i]; // Accumulate A magnitude squared
    magnitudeB += vectorB[i] * vectorB[i]; // Accumulate B magnitude squared
  }

  // Calculate actual magnitude (square root)
  magnitudeA = Math.sqrt(magnitudeA);
  magnitudeB = Math.sqrt(magnitudeB);

  // Handle zero vector case (avoid division by zero)
  if (magnitudeA === 0 || magnitudeB === 0) {
    return 0;
  }

  // Cosine similarity formula: cos(θ) = (A·B) / (|A|×|B|)
  return dotProduct / (magnitudeA * magnitudeB);
}

/**
 * Export system constants
 *
 * Key configuration parameters for external modules to ensure system-wide consistency.
 *
 * • EMBEDDING_DIMENSION: Standard vector dimension for text-embedding-ada-002 (1536)
 * • MAX_CHUNK_SIZE: Maximum character limit per text chunk based on token limits
 */
export { EMBEDDING_DIMENSION, MAX_CHUNK_SIZE };
